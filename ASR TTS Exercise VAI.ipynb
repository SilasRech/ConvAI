{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953eca12-4e7e-4226-9e79-03747a1d14cd",
   "metadata": {},
   "source": [
    "Demo File for Speech Recognition and Speech Synthesis use Cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a36bd38-f540-4a5b-9d77-86f50af2464a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip -q install -U openai-whisper\n",
    "!pip -q install wavio\n",
    "!pip -q install scipy\n",
    "!pip -q install sounddevice\n",
    "!pip -q install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2366f-903c-4524-8c09-c67d5aa5d480",
   "metadata": {},
   "source": [
    "Introducing Whisper for non-real time transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da8c42-30d4-4f82-bac9-53269ac4df4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import wavio as wv\n",
    "\n",
    "# Hi this is a sample recording of my voice and to see how well the speech recognition is working.\n",
    "\n",
    "# Sampling frequency\n",
    "freq = 16000\n",
    " \n",
    "# Recording duration\n",
    "duration = 5\n",
    " \n",
    "# Start recorder with the given values \n",
    "# of duration and sample frequency\n",
    "recording = sd.rec(int(duration * freq), \n",
    "                   samplerate=freq, channels=2)\n",
    "\n",
    "print('Recording Audio...') \n",
    "# Record audio for the given number of seconds\n",
    "sd.wait()\n",
    " \n",
    "# This will convert the NumPy array to an audio\n",
    "# file with the given sampling frequency\n",
    "write(\"recording0.wav\", freq, recording)\n",
    "\n",
    "# Use internal transcription definition (Quick Way)\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"recording0.wav\")\n",
    "print(result[\"text\"])\n",
    "\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"recording0.wav\")\n",
    "\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(\n",
    "    task = \"transcribe\",\n",
    "\n",
    "    # language that the audio is in; uses detected language if None\n",
    "    language= max(probs, key=probs.get),\n",
    "\n",
    "    # sampling-related options\n",
    "    temperature = 0.0,\n",
    "    sample_len = None,  # maximum number of tokens to sample\n",
    "    best_of = None,  # number of independent sample trajectories, if t > 0\n",
    "    beam_size = None,  # number of beams in beam search, if t == 0\n",
    "    patience = None,  # patience in beam search (arxiv:2204.05424)\n",
    "\n",
    "    # text or tokens to feed as the prompt or the prefix; for more info:\n",
    "    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051\n",
    "    prompt = None,  # for the previous context\n",
    "    prefix = None,  # to prefix the current context\n",
    "\n",
    "    # list of tokens ids (or comma-separated token ids) to suppress\n",
    "    # \"-1\" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`\n",
    "    suppress_tokens = \"-1\",\n",
    "    suppress_blank = True,  # this will suppress blank outputs\n",
    "\n",
    "    # timestamp sampling options\n",
    "    without_timestamps = False,  # use <|notimestamps|> to sample text tokens only\n",
    "    max_initial_timestamp = 1.0,\n",
    "\n",
    "    # implementation details\n",
    "    fp16 = False,  # use fp16 for most of the calculation\n",
    ")\n",
    "                                  \n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29743586-d8cf-47f4-bd2c-38ee9d6e761e",
   "metadata": {},
   "source": [
    "Introducing Emformer for real-time transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885f881-79e6-4638-b52a-22f2d2c6c0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Requirements \n",
    "!pip -q install torch\n",
    "!pip -q install torchaudio\n",
    "!pip -q install SentencePiece\n",
    "\n",
    "# Might need to conda install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a4a816-0322-42e7-908f-0e533bb98284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio \n",
    "\n",
    "class ContextCacher:\n",
    "    \"\"\"Cache the end of input data and prepend the next input data with it.\n",
    "\n",
    "    Args:\n",
    "        segment_length (int): The size of main segment.\n",
    "            If the incoming segment is shorter, then the segment is padded.\n",
    "        context_length (int): The size of the context, cached and appended.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment_length: int, context_length: int):\n",
    "        self.segment_length = segment_length\n",
    "        self.context_length = context_length\n",
    "        self.context = torch.zeros([context_length])\n",
    "\n",
    "    def __call__(self, chunk: torch.Tensor):\n",
    "        if chunk.size(0) < self.segment_length:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n",
    "        chunk_with_context = torch.cat((self.context, chunk))\n",
    "        self.context = chunk[-self.context_length :]\n",
    "        return chunk_with_context\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc5fdb-fe35-45d4-87f4-c8de66ada4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchaudio.io import StreamReader\n",
    "\n",
    "src = \"recording0.wav\"\n",
    "\n",
    "# Get pipeline\n",
    "bundle = torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\n",
    "\n",
    "feature_extractor = bundle.get_streaming_feature_extractor()\n",
    "decoder = bundle.get_decoder()\n",
    "token_processor = bundle.get_token_processor()\n",
    "\n",
    "sample_rate = bundle.sample_rate\n",
    "segment_length = bundle.segment_length * bundle.hop_length\n",
    "context_length = bundle.right_context_length * bundle.hop_length\n",
    "\n",
    "# Stream Audio File\n",
    "streamer = StreamReader(src)\n",
    "streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=bundle.sample_rate)\n",
    "\n",
    "state, hypothesis = None, None    \n",
    "cacher = ContextCacher(segment_length, context_length)\n",
    "\n",
    "stream_iterator = streamer.stream()\n",
    "\n",
    "# Run speech recognition\n",
    "@torch.inference_mode()\n",
    "def run_inference(num_iter=1000):\n",
    "    global state, hypothesis\n",
    "    chunks = []\n",
    "    feats = []\n",
    "    for i, (chunk,) in enumerate(stream_iterator, start=1):\n",
    "        segment = cacher(chunk[:, 0])\n",
    "        features, length = feature_extractor(segment)\n",
    "        hypos, state = decoder.infer(features, length, 10, state=state, hypothesis=hypothesis)\n",
    "        hypothesis = hypos\n",
    "        transcript = token_processor(hypos[0][0], lstrip=False)\n",
    "        print(transcript, end=\"\\r\", flush=True)\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        feats.append(features)\n",
    "        if i == num_iter:\n",
    "            break\n",
    "\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fa12c-e39a-4b2b-8848-f8904c43101a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Speech Synthesis Tacotron2 and HifiGan \n",
    "\n",
    "More Links to check out:\n",
    "https://google.github.io/tacotron/publications/tacotron2/index.html\n",
    "https://github.com/suno-ai/bark\n",
    "https://github.com/coqui-ai/TTS\n",
    "\n",
    "Speech Brain\n",
    "https://github.com/speechbrain/speechbrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517dfb9-5d5f-4267-8f8d-db5ae7ad8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 -q install deep_phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1765d4-443e-4a04-98e1-b19cdb9cfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1430873-a3ea-4c24-974e-038101c23c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silas\\miniconda3\\envs\\jup\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m bundle \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mpipelines\u001b[38;5;241m.\u001b[39mTACOTRON2_WAVERNN_PHONE_LJSPEECH\n\u001b[0;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m bundle\u001b[38;5;241m.\u001b[39mget_text_processor()\n\u001b[1;32m----> 4\u001b[0m tacotron2 \u001b[38;5;241m=\u001b[39m bundle\u001b[38;5;241m.\u001b[39mget_tacotron2()\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m      5\u001b[0m vocoder \u001b[38;5;241m=\u001b[39m bundle\u001b[38;5;241m.\u001b[39mget_vocoder()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "symbols = \"_-!'(),.:;? abcdefghijklmnopqrstuvwxyz\"\n",
    "look_up = {s: i for i, s in enumerate(symbols)}\n",
    "symbols = set(symbols)\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    text = text.lower()\n",
    "    return [look_up[s] for s in text if s in symbols]\n",
    "\n",
    "text = \"This is a response text and to test how well tacotron is working.\"\n",
    "print(text_to_sequence(text))\n",
    "print([processor.tokens[i] for i in processed[0, : lengths[0]]])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    waveforms, lengths = vocoder(spec, spec_lengths)\n",
    "\n",
    "plot(waveforms, spec, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720b456-33c3-4bf1-93dd-cfedc19e8a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
